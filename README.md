# ATRT: Attention-Regularized Transformer for Text Classification

A lightweight Transformer model built from scratch in TensorFlow with a custom attention mechanism designed to improve convergence and training stability. The model was tested on the IMDb movie reviews dataset for binary sentiment classification.

## âœ¨ Highlights

- âœ… **Custom Transformer Architecture** built from scratch using TensorFlow
- ğŸ§  **Gaussian-Scaled Multi-Head Attention** for improved focus and early convergence
- ğŸ“ˆ Achieved **78% test accuracy** on the IMDb dataset
- âš™ï¸ Implemented **dynamic positional encoding**, dropout, and regularization
- ğŸ§ª Modular and readable code using **TensorFlow Datasets** and `TextVectorization`

## ğŸ—‚ï¸ Dataset

- **IMDb Reviews Dataset** (via `tensorflow_datasets`)
- Binary sentiment classification: Positive or Negative

## ğŸ—ï¸ Model Architecture

- Embedding + Positional Encoding  
- Multiple Encoder Layers with:
  - Gaussian-Scaled Multi-Head Attention
  - Layer Normalization and Dropout
  - Feedforward Neural Networks  
- GlobalAveragePooling + Dense Sigmoid Output

## ğŸ”¬ Custom Attention: ATRT

Introduces a **Gaussian-based scaling** to standard attention weights, encouraging smoother and more stable convergence while preserving performance.



